import os
import json
import PyPDF2
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
import tiktoken
from langchain.prompts import PromptTemplate

from utils.SemanticSplitter import SemanticSplitter
from dotenv import load_dotenv

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

from typing import List, Dict
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser

# For multiple-choice quizzes
class ChoiceQuestion(BaseModel):
    question: str = Field(..., description="Question generated by the llm using the pdf")
    options: List[str]
    answer: str
    explanation: str = Field(..., description="Explanation as to why the answer is correct")

class QuizChoice(BaseModel):
    questions: List[ChoiceQuestion]

# For paragraph quizzes
class QuizParagraph(BaseModel):
    questions: Dict[str, str]

class GradeResult(BaseModel):
    grade: int  = Field(..., description="Grade out of 10")
    improvements: str = Field(..., description="Feedback on how to improve the answer")


class QuizGenerator:
    def __init__(self):
        # Use ChatOpenAI (gpt-4o-mini) for *all* LLM calls including summarization.
        self.llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.6, openai_api_key=OPENAI_API_KEY)
        self.splitter = SemanticSplitter()

    def extract_text_from_pdf(self, pdf_path):
        from PyPDF2 import PdfReader
        
        reader = PdfReader(pdf_path)
        text = " ".join([page.extract_text() for page in reader.pages if page.extract_text()])
        return text

    def count_tokens(self, text, model="llama-3.1-8b-instant"):
        try:
            enc = tiktoken.get_encoding("cl100k_base")
            return len(enc.encode(text))
        except Exception as e:
            print(f"Token count error: {e}")
            return len(text.split())


    def generate_quiz(self, pdf_path, difficulty, mode, num_questions):
        text = self.extract_text_from_pdf(pdf_path)
        token_count = self.count_tokens(text)
        
        # When the text is very large, split it semantically and summarize each chunk using the LLM.
        if token_count > 5000:
            print("Large text detected. Chunking and summarizing with ChatOpenAI...")
            chunks = self.splitter.split_transcript(text)
            final_text = " ".join(chunk.page_content for chunk in chunks)
        else:
            final_text = text
        if mode == "choice":
            parser = JsonOutputParser(pydantic_object=QuizChoice)
        else:
            parser = JsonOutputParser(pydantic_object=QuizParagraph)

        format_instructions = parser.get_format_instructions()
        
        template = """
            You are an expert quiz generator. Given the following text:
            --------------------------------------------------
            {final_text}
            --------------------------------------------------

            Generate a {num_questions}-question quiz using the content above with a difficulty level of "{difficulty}" in "{mode}" mode.

            **Requirements:**
            1. Produce exactly {num_questions} questions.
            2. Each question must be complete, descriptive, and derived directly from the provided text. Do not use generic placeholders like "Question 1", "Question 2", etc.
            3. Depending on the mode, return the output strictly as a JSON object in one of the following formats:

            If the mode is "paragraph", return the JSON object in this format:
            {{
            "questions": {{
                "Complete and detailed question text 1?": "",
                "Complete and detailed question text 2?": "",
                ...
                "Complete and detailed question text {num_questions}?": ""
            }}
            }}

            If the mode is "choice", return the JSON object in this format:
            {{
            "questions": [
                {{
                "question": "Complete and detailed question text 1?",
                "options": ["Option A", "Option B", "Option C", "Option D"],
                "answer": "Correct Option",
                "explanation": "Explanation for why the answer is correct."
                }},
                {{
                "question": "Complete and detailed question text 2?",
                "options": ["Option A", "Option B", "Option C", "Option D"],
                "answer": "Correct Option",
                "explanation": "Explanation for why the answer is correct."
                }},
                ...
                {{
                "question": "Complete and detailed question text {num_questions}?",
                "options": ["Option A", "Option B", "Option C", "Option D"],
                "answer": "Correct Option",
                "explanation": "Explanation for why the answer is correct."
                }}
            ]
            }}

            4. Return only the JSON object and nothing elseâ€”no explanations, comments, or additional text.

            Important: Ensure that in every case the keys (or the "question" field in choice mode) contain the full, detailed question text extracted from the provided content.

            {format_instructions}
            """

        prompt_template = PromptTemplate(
                input_variables=["final_text", "num_questions", "difficulty", "mode", "format_instructions"],
                template=template
            )
        
        formatted_prompt = prompt_template.format(
            final_text=final_text,
            num_questions=num_questions,
            difficulty=difficulty,
            mode=mode,
            format_instructions=format_instructions
        )
        try:
            response = self.llm.invoke([HumanMessage(content=formatted_prompt)]).content
            quiz_data = parser.parse(response)
        except Exception as e:
            print("Error generating quiz:", e)
            quiz_data = {"error": "Invalid JSON response from AI"}
        
        return quiz_data

    def grade_paragraph_answers(self, user_answers):
        graded_responses = {}

        for question, answer in user_answers.items():
            parser = JsonOutputParser(pydantic_object=GradeResult)
            format_instructions = parser.get_format_instructions()

            prompt = f"""
                Grade this answer out of 10. Be strict with the grading :

                Question: {question}
                Answer: {answer}

                {format_instructions}
            """
            response = self.llm.invoke([HumanMessage(content=prompt)]).content

            try:
                graded_responses[question] = parser.parse(response)
            except Exception as e:
                print(f"Error grading question '{question}':", e)
                graded_responses[question] = {"grade": 0, "improvements": "Unable to evaluate."}

        return graded_responses
